<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="Truth-Decay" class="title is-3 publication-title">Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery
            </h1>
            <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=sv7uxi4AAAAJ&hl=en" target="_blank">Yifan Sun<sup>1,2</sup></a>,</span>

                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=hGZwK0cAAAAJ&hl=en" target="_blank">Danding Wang<sup>1</sup></a>,</span>
  
                  <span class="author-block">
                  <a href="https://sheng-qiang.github.io/" target="_blank">Qiang Sheng<sup>1</sup></a>,</span>
  
                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=fSBdNg0AAAAJ&hl=zh-CN&oi=ao" target="_blank">Juan Cao<sup>1,2</sup></a>,</span>
                  
                  <span class="author-block">
                  <a href="mailto:jtli@ict.ac.cn" target="_blank">Jintao Li<sup>1,2</sup></a>,</span>
              
                  </span>
                  
                  </div>
                  
                  <div class="is-size-5 publication-authors">
                  <span class="author-block">
                      <sup>1</sup>Institute of Computing Technology, Chinese Academy of Sciences<br>
                      <sup>2</sup>University of Chinese Academy of Sciences<br>
                    </span>
  
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2505.20293" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.20293" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

              </div>
            </div>
            <!-- <div class="is-size-5 publication-authors">
              Published at <b><i>ACL 2025 Findings</i></b>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="section">

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/short_cut/intro.png" style="width:100%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Concept-based explainable approaches have emerged as a promising method in explainable AI because they can interpret models in a way that aligns with human reasoning. However, their adaption in the text domain remains limited. Most existing methods rely on predefined concept annotations and cannot discover unseen concepts, while other methods that extract concepts without supervision often produce explanations that are not intuitively comprehensible to humans, potentially diminishing user trust. These methods fall short of discovering comprehensible concepts automatically. To address this issue, we propose ECO-Concept, an intrinsically interpretable framework to discover comprehensible concepts with no concept annotations. ECO-Concept first utilizes an object-centric architecture to extract semantic concepts automatically. Then the comprehensibility of the extracted concepts is evaluated by large language models. Finally, the evaluation result guides the subsequent model fine-tuning to obtain more understandable explanations. Experiments show that our method achieves superior performance across diverse tasks. Further concept evaluations validate that the concepts learned by ECO-Concept surpassed current counterparts in comprehensibility.</p>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <div style="text-align:center">
        <h2 class="title is-3">Method</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>
            ECO-Concept consists of three modules: a concept extractor, a classifier, and
            a concept evaluator. The concept extractor takes the encoded text as input and generates a concept slot
            attention matrix by interacting with the concept prototypes. The slot attention scores across tokens are then
            summed and fed into the classifier for prediction. To enhance the comprehensibility of the concepts, we
            designed a concept evaluator that receives slot attention from the concept extractor, maps them back to the
            original text, and uses human proxies (here, LLMs)1 to summarize the concepts, and highlights concept
            related segments with additional exemplars. If a concept is easy to understand, the highlighted segments
            should be similar to the model’s slot attention. We use the difference between the highlighted segments and
            the model’s slot attention scores, considering the importance of each concept, as a comprehensibility loss.
            This feedback is used to make the learned concepts easier for human understanding.
        </p>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/short_cut/framework.png" alt="Figure 1" style="width:90%; display:block; margin:0 auto;">
          </figure>
      </div>
      <br>
      <br>


      <div style="text-align:center">
        <h2 class="title is-3">ECO-Concept achieves interpretability without compromising classification performance.</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>
            ECO-Concept achieves superior classification performance across various datasets. Compared to supervised methods, ECO-Concept achieves competitive results with no concept supervision, indicating its ability to automatically discover new concepts without compromising performance. Compared to unsupervised baselines, ECO-Concept shows significant improvements in both accuracy and F1 with pairwise t-tests at a 95% confidence level, validating the effectiveness of its conceptual representations. Moreover, ECO-Concept also has comparable or better performance compared with black-box models. This indicates that it effectively balances both task-discriminativity and concept comprehensibility, showing the potential to build interpretable models without performance trade-offs.
        </p>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/short_cut/classification_performence.png" alt="Figure 2" style="width:80%; display:block; margin:0 auto;">
          </figure>
      </div>
      <br>
      <br>



      <div style="text-align:center">
        <h2 class="title is-3">ECO-Concept learnes more comprehensible concept explanations.</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>
            To evaluate the comprehensibility of the concepts extracted by our method, we first define three quantitative metrics (Semantics, Distinctiveness, Consistency).
            Then we conduct several human evaluations, including intruder detection, subjective ratings, and forward simulatability to further assess how easily these concepts can be understood.
            Our ECO-Concept outperforms existing concept-based approaches in both quantitative metrics and user studies, demonstrating its effectiveness in generating human-aligned, interpretable explanations.
        </p>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/short_cut/concept_performance.png" alt="Figure 3" style="width:80%; display:block; margin:0 auto;">
          </figure>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>
           We conducted subjective rating experiments, where participants rated all the extracted concepts from multiple perspectives: Consistency, Clarity, Task Relevance, and Comprehensibility. Our method consistently achieved the highest ratings across all perspectives for each task. This result indicates that our concepts are subjectively the most comprehensible to humans and are closely aligned with the tasks. In contrast, ProtoTEx received the lowest ratings in most tasks, suggesting that traditional unsupervised self-explaining methods often struggle to balance interpretability with task performance, resulting in less intuitive concepts for human understanding.
        </p>
      </div>
        <div class="columns is-centered">
            <figure>
                <img src="static/short_cut/human_ratings.png" alt="Figure 4" style="width:80%; display:block; margin:0 auto;">
            </figure>
        </div>
      <br>
        <div class="columns is-centered">
            <figure>
                <img src="static/short_cut/cases.png" alt="Figure 5" style="width:80%; display:block; margin:0 auto;">
            </figure>
        </div>
      <br>
      <br>

      <!--/ Method -->
        </div>
      </div>

    </div>
  </section>
  
<!--   <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
     <pre>
      <code>
        Coming soon!
      </code>
    </pre>
    <pre><code>@inproceedings{truthdecay,
      title={{LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation}},
      author={Hu, Beizhe and Sheng, Qiang and Cao, Juan and Li, Yang and Wang, Danding},
      booktitle={Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
      doi={10.1145/3726302.3730027},
      year={2025}}</code></pre>
  </div>
</section>  -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              The primary code for this webpage is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a>, <a href="https://beanandrew.github.io/projects/TruthDecay">TruthDecay</a> and <a href="https://github.com/llm-misinformation/llm-misinformation.github.io/">LLM-Misinformation</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</html>
